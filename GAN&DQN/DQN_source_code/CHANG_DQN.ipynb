{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cuda:4\n",
      "Episode:      1, interaction_steps:   2048, reward:  8, epsilon: 0.996314\n",
      "[Info] Save model at 'model' !\n",
      "Evaluation: True, episode:      1, interaction_steps:   2048, evaluate reward:  0\n",
      "Episode:      2, interaction_steps:   4096, reward: 10, epsilon: 0.992627\n",
      "Episode:      3, interaction_steps:   6144, reward: 12, epsilon: 0.988941\n",
      "Episode:      4, interaction_steps:   8192, reward: 10, epsilon: 0.985254\n",
      "Episode:      5, interaction_steps:  10240, reward: 12, epsilon: 0.981568\n",
      "Episode:      6, interaction_steps:  12288, reward: 12, epsilon: 0.977882\n",
      "Episode:      7, interaction_steps:  14336, reward: 11, epsilon: 0.974195\n",
      "Episode:      8, interaction_steps:  16384, reward: 12, epsilon: 0.970509\n",
      "Episode:      9, interaction_steps:  18432, reward: 12, epsilon: 0.966822\n",
      "Episode:     10, interaction_steps:  20480, reward: 12, epsilon: 0.963136\n",
      "Episode:     11, interaction_steps:  22528, reward: 10, epsilon: 0.959450\n",
      "Evaluation: True, episode:     11, interaction_steps:  22528, evaluate reward:  0\n",
      "Episode:     12, interaction_steps:  24576, reward:  9, epsilon: 0.955763\n",
      "Episode:     13, interaction_steps:  26624, reward: 12, epsilon: 0.952077\n",
      "Episode:     14, interaction_steps:  28672, reward:  9, epsilon: 0.948390\n",
      "Episode:     15, interaction_steps:  30720, reward:  9, epsilon: 0.944704\n",
      "Episode:     16, interaction_steps:  32768, reward: 11, epsilon: 0.941018\n",
      "Episode:     17, interaction_steps:  34816, reward: 10, epsilon: 0.937331\n",
      "Episode:     18, interaction_steps:  36864, reward: 13, epsilon: 0.933645\n",
      "Episode:     19, interaction_steps:  38912, reward: 10, epsilon: 0.929958\n",
      "Episode:     20, interaction_steps:  40960, reward:  9, epsilon: 0.926272\n",
      "Episode:     21, interaction_steps:  43008, reward: 12, epsilon: 0.922586\n",
      "Evaluation: True, episode:     21, interaction_steps:  43008, evaluate reward:  0\n",
      "Episode:     22, interaction_steps:  45056, reward: 10, epsilon: 0.918899\n",
      "Episode:     23, interaction_steps:  47104, reward: 11, epsilon: 0.915213\n",
      "Episode:     24, interaction_steps:  49152, reward:  9, epsilon: 0.911526\n",
      "Episode:     25, interaction_steps:  51200, reward: 11, epsilon: 0.907840\n",
      "Episode:     26, interaction_steps:  53248, reward: 11, epsilon: 0.904154\n",
      "Episode:     27, interaction_steps:  55296, reward: 11, epsilon: 0.900467\n",
      "Episode:     28, interaction_steps:  57344, reward:  9, epsilon: 0.896781\n",
      "Episode:     29, interaction_steps:  59392, reward: 11, epsilon: 0.893094\n",
      "Episode:     30, interaction_steps:  61440, reward: 10, epsilon: 0.889408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4d092dd4d855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;31m#assert False, \"You should check the source code for your homework!!\" # Before you run function, please check hyperparameters again!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4d092dd4d855>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4d092dd4d855>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mnext_state_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# calculate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Use device: %s\"%device)\n",
    " \n",
    "# Please tune the hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_ep\", default=800, type=int)\n",
    "parser.add_argument(\"--mem_capacity\", default=90000, type=int) # In paper, size is 1000000\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int) # 16 32 64 128\n",
    "parser.add_argument(\"--lr\", default=0.0003, type=float) # ~0.0001 ~0.001\n",
    "parser.add_argument(\"--gamma\", default=0.999, type=float) # 0.8 0.9 0.99 0.999\n",
    "parser.add_argument(\"--epsilon_start\", default=1.0, type=float)\n",
    "parser.add_argument(\"--epsilon_final\", default=0.1, type=float) # Try 0.05?\n",
    "parser.add_argument(\"--epsilon_decay\", default=500000, type=float) # 500000 1000000 4000000 \n",
    "parser.add_argument(\"--target_step\", default=10000, type=int) # ?\n",
    "parser.add_argument(\"--eval_per_ep\", default=10, type=int)\n",
    "parser.add_argument(\"--save_per_ep\", default=50, type=int)\n",
    "parser.add_argument(\"--save_dir\", default=\"model\")\n",
    "parser.add_argument(\"--log_file\", default=\"log.txt\") # you can plot the figure accroding to the file\n",
    "parser.add_argument(\"--load_model\", default=None)\n",
    "parser.add_argument(\"--train\", default=True, type=bool)\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        #assert False, \"You should check the source code for your homework!!\" # Sample when you are training\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        #assert False, \"You should check the source code for your homework!!\" # In original paper, there is no batch normalization and leaky relu.\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,8,4),4,2),3,1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,8,4),4,2),3,1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "        self.fc = nn.Linear(linear_input_size, 512)\n",
    "        self.head = nn.Linear(512, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.fc(x.view(x.size(0), -1)))\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "        self.GAMMA = args.gamma\n",
    "        self.EPS_START = args.epsilon_start\n",
    "        self.EPS_END = args.epsilon_final\n",
    "        self.EPS_DECAY = args.epsilon_decay\n",
    "        self.LEARN_RATE = args.lr\n",
    "\n",
    "        self.action_dim = 3\n",
    "        self.state_dim = (84,84)\n",
    "        self.epsilon = 0.0\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.policy_net = CNN(84, 84, self.action_dim).to(device)\n",
    "        self.target_net = CNN(84, 84, self.action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr=self.LEARN_RATE)\n",
    "        \n",
    "        self.memory = ReplayMemory(args.mem_capacity)\n",
    "        self.interaction_steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.interaction_steps += 1\n",
    "        self.epsilon = self.EPS_END + np.maximum( (self.EPS_START-self.EPS_END) * (1 - self.interaction_steps/self.EPS_DECAY), 0) # linear decay\n",
    "        if random.random() < self.epsilon:\n",
    "            #assert False, \"You should check the source code for your homework!!\" # random select for epsilon greedy (Dependent on the exploration probability)\n",
    "            return torch.tensor( [np.random.choice([0,1,2], 1, p=[0.3,0.6,0.1])], device=device, dtype=torch.long ) #torch.tensor([[random.randrange(self.action_dim)]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    def evaluate_action(self, state, rand=0.1):\n",
    "        if random.random() < rand:\n",
    "            #assert False, \"You should check the source code for your homework!!\" # random select for evaluate action should be the final epsilon in training\n",
    "            return torch.tensor( [np.random.choice([0,1,2], 1, p=[0.3,0.6,0.1])], device=device, dtype=torch.long )\n",
    "        with torch.no_grad():\n",
    "            return self.target_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            print(\"[Warning] Memory data less than batch sizes!\")\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        #final_mask = torch.cat(batch.done)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        #final_mask = torch.cat(batch.done).to(device) #\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(batch.action).to(device) \n",
    "        reward_batch = torch.cat(batch.reward).to(device) \n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        # calculate \n",
    "        # the\n",
    "        # loss\n",
    "        # function\n",
    "        # here\n",
    "        # and\n",
    "        # backward\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch) #Q(s_t, a_t)\n",
    "        #next_state_values = self.target_net(non_final_next_states).detach() #Q(s_t+1, a_t)\n",
    "        expected_state_action_values = (next_state_values.view(-1,1) * self.GAMMA) + reward_batch # alpha(R_t+gamma*maxQ)\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        #assert False, \"You should check the source code for your homework!!\" # please check these pytorch and do regression problem\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_count += 1\n",
    "        if self.update_count % args.target_step == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        with torch.no_grad():\n",
    "            #assert False, \"You should check the source code for your homework!!\" # why need to do this function? Which update is better?\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "# =============================================================================\n",
    "#             for q, q_targ in zip(self.policy_net.parameters(), self.target_net.parameters()):\n",
    "#                 q_targ.data.mul_(0.05)\n",
    "#                 q_targ.data.add_(0.95 * q.data)\n",
    "#             self.target_net.eval()\n",
    "# =============================================================================\n",
    "\n",
    "    def save_model(self, path=\".\"):\n",
    "        torch.save(self.target_net.state_dict(), path+'/q_target_checkpoint_{}.pth'.format(self.interaction_steps))\n",
    "        torch.save(self.policy_net.state_dict(), path+'/q_policy_checkpoint_{}.pth'.format(self.interaction_steps))\n",
    "\n",
    "    def restore_model(self, path):\n",
    "        self.target_net.load_state_dict(torch.load(path))\n",
    "        self.policy_net.load_state_dict(torch.load(path))\n",
    "        self.target_net.eval()\n",
    "        print(\"[Info] Restore model from '%s' !\"%path)\n",
    "\n",
    "class RandomAgent(object):\n",
    "    def __init__(self):\n",
    "        self.action_dim = 3\n",
    "        self.interaction_steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.interaction_steps += 1\n",
    "        return torch.tensor( [random.sample([0,1,2],1)], device=device, dtype=torch.long ) # random.sample([0,0,0,1,1,1,1,1,1,2],1)\n",
    "\n",
    "    def evaluate_action(self, state):\n",
    "        return torch.tensor( [random.sample([0,1,2],1)], device=device, dtype=torch.long )\n",
    "\n",
    "frame_proc = T.Compose([T.ToPILImage(),\n",
    "                        T.Grayscale(), \\\n",
    "                        T.Resize((84,84), interpolation=Image.BILINEAR), \\\n",
    "                        T.ToTensor()])\n",
    "\n",
    "class Atari(object):\n",
    "    def __init__(self, env_name=\"FreewayDeterministic-v4\", agent_history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state = None\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        frame = self.image_proc(observation).to(device)\n",
    "        self.state = frame.repeat(1,4,1,1)\n",
    "        return self.state\n",
    "\n",
    "    def image_proc(self, image):\n",
    "        return frame_proc(image)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        frame = self.image_proc(observation).to(device)\n",
    "        next_state = torch.cat( (self.state[:, 1:, :, :], frame.unsqueeze(0)), axis=1 )\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        observation = self.env.render(mode='rgb_array')\n",
    "        return observation\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_episodes = args.train_ep\n",
    "    save_model_per_ep = args.save_per_ep\n",
    "    log_fd = open(args.log_file,'w')\n",
    "\n",
    "    ########## Training ##########\n",
    "    agent = DQN()\n",
    "    env = Atari()\n",
    "\n",
    "    if args.load_model:\n",
    "        agent.restore_model(args.load_model)\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    \n",
    "    global_steps = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for _ in range(10000):\n",
    "            env.render()\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            if done:\n",
    "                next_state  = None\n",
    "\n",
    "            #assert False, \"You should check the source code for your homework!!\" # Here put the transition data on device you want and may convert when update\n",
    "            agent.memory.push(  state, \\\n",
    "                                action, \\\n",
    "                                next_state, \\\n",
    "                                torch.tensor([[reward]], device=device), \\\n",
    "                                torch.tensor([done], device=device, dtype=torch.bool))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            global_steps += 1 \n",
    "\n",
    "            if global_steps > 200:\n",
    "                agent.update()\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\"%(i_episode+1, agent.interaction_steps, episode_reward, agent.epsilon))\n",
    "                log_fd.write(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\\n\"%(i_episode+1, agent.interaction_steps, episode_reward, agent.epsilon))\n",
    "                break\n",
    "        \n",
    "\n",
    "        if i_episode % save_model_per_ep == 0:\n",
    "            agent.save_model(args.save_dir)\n",
    "            print(\"[Info] Save model at '%s' !\"%args.save_dir)\n",
    "\n",
    "        if i_episode % args.eval_per_ep == 0:\n",
    "            test_env = Atari()\n",
    "            test_times = 5\n",
    "\n",
    "            average_reward = 0.0\n",
    "            for t_ep in range(test_times):\n",
    "                episode_reward = 0.0\n",
    "                state = test_env.reset()\n",
    "                for _ in range(10000):\n",
    "                    action = agent.evaluate_action(state)\n",
    "                    state, reward, done, _ = test_env.step(action.item())\n",
    "                    episode_reward += reward\n",
    "            average_reward += episode_reward\n",
    "            \n",
    "            print(\"Evaluation: True, episode: %6d, interaction_steps: %6d, evaluate reward: %2d\"%(i_episode+1, agent.interaction_steps, average_reward/test_times))\n",
    "            log_fd.write(\"Evaluation: True, episode: %6d, interaction_steps: %6d, evaluate reward: %2d\"%(i_episode+1, agent.interaction_steps, average_reward/test_times))\n",
    "    \n",
    "    log_fd.close()\n",
    "\n",
    "def test():\n",
    "    agent = DQN() # agent = RandomAgent()\n",
    "    env = Atari()\n",
    "    test_epsilon = args.epsilon_final\n",
    "    \n",
    "    if args.load_model:\n",
    "        agent.restore_model(args.load_model)\n",
    "    else:\n",
    "        test_epsilon = 1.0\n",
    "      \n",
    "\n",
    "    for i_episode in range(10):\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for _ in range(10000):\n",
    "            env.render() # For Desktop window\n",
    "            action = agent.evaluate_action(state, test_epsilon)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\"%(i_episode, agent.interaction_steps, episode_reward, test_epsilon))\n",
    "                break\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#args = parser.parse_args()  #For Terminal\n",
    "args = parser.parse_args(args=[]) # For jupyter notebook\n",
    "#assert False, \"You should check the source code for your homework!!\" # Before you run function, please check hyperparameters again!!\n",
    "if args.train:\n",
    "    main()\n",
    "else:\n",
    "    test()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
