{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from gensim.models import word2vec, fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pd.read_csv('train_data.csv')\n",
    "test_text = pd.read_csv('test_data.csv')\n",
    "total_text = pd.concat([train_text['title'], test_text['title']], axis=0)\n",
    "total_key = pd.concat([train_text['keyword'], test_text['keyword']], axis=0)\n",
    "total_text = total_text.reset_index(drop=True)\n",
    "total_key = total_key.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "40000\n",
      "45000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "110000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "165000\n",
      "185000\n",
      "190000\n",
      "205000\n",
      "210000\n",
      "215000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "245000\n",
      "250000\n",
      "260000\n",
      "275000\n",
      "280000\n",
      "290000\n"
     ]
    }
   ],
   "source": [
    "total_keydrop = total_key.dropna(axis=0)  \n",
    "keyword = []\n",
    "for i in total_keydrop.index:\n",
    "    if i%5000 == 0:\n",
    "        print(i)\n",
    "    keyword = keyword + total_key[i].split(',')\n",
    "##userDict.txt\n",
    "file = open('userDict.txt','w', encoding=\"utf-8\")\n",
    "file.writelines([\"%s\\n\" % item for item in keyword])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.648 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 中文停用詞\n",
    "stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "# 自定義中文詞彙\n",
    "jieba.load_userdict('userDict.txt')\n",
    "# 定義中文分詞，包含去除掉停用詞\n",
    "def title_preprocessing(train_text):\n",
    "    all_words = list(jieba.cut(train_text, cut_all=False))\n",
    "    fileTrainSeg = []\n",
    "    for word in all_words:\n",
    "        if word not in stopwords:\n",
    "            fileTrainSeg.append(word)\n",
    "    return(fileTrainSeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299537,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_keydot = total_key.fillna(',')\n",
    "corpus= total_text.astype('str').apply(title_preprocessing)\n",
    "corpuskey = total_keydot.astype('str').apply(title_preprocessing)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcorpus = corpus + corpuskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#詞庫有多少詞彙\n",
    "MAX_NUM_WORDS = 100000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞集&轉成vector\n",
    "tokenizer.fit_on_texts(newcorpus)\n",
    "X_total = tokenizer.texts_to_sequences(newcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一個標題最長有幾個詞彙\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "X_total = keras.preprocessing.sequence.pad_sequences(X_total, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = keras.utils.to_categorical(train_text['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_total[:len(train_text)]\n",
    "X_test = X_total[len(train_text):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# num_folds = 2\n",
    "# kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1873/1873 [==============================] - 305s 163ms/step - loss: 0.4842 - accuracy: 0.8537\n",
      "Epoch 2/5\n",
      "1873/1873 [==============================] - 304s 162ms/step - loss: 0.1609 - accuracy: 0.9539\n",
      "Epoch 3/5\n",
      "1873/1873 [==============================] - 307s 164ms/step - loss: 0.0945 - accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "1873/1873 [==============================] - 298s 159ms/step - loss: 0.0639 - accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "1873/1873 [==============================] - 313s 167ms/step - loss: 0.0471 - accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam()\n",
    "loss_function = categorical_crossentropy\n",
    "#for train, test in kfold.split(X_train, train_text['label']):\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 128\n",
    "\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 64\n",
    "\n",
    "# 是否顯示進度條\n",
    "verbosity = 1\n",
    "\n",
    "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
    "from keras import Input\n",
    "from keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# 分別定義 2 個新聞標題 A & B 為模型輸入\n",
    "# 兩個標題都是一個長度為 20 的數字序列\n",
    "train_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "# 詞嵌入層\n",
    "# 經過詞嵌入層的轉換，兩個新聞標題都變成\n",
    "# 一個詞向量的序列，而每個詞向量的維度\n",
    "# 為 64\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "train_embedded = embedding_layer(train_input)\n",
    "\n",
    "# LSTM 層\n",
    "# 兩個新聞標題經過此層後\n",
    "# 為一個 64 維度向量\n",
    "shared_lstm1 = LSTM(NUM_LSTM_UNITS, return_sequences = True, dropout=0.2)\n",
    "train_lstm1 = shared_lstm1(train_embedded)\n",
    "shared_lstm2 = LSTM(NUM_LSTM_UNITS, dropout=0.2)\n",
    "train_lstm2 = shared_lstm2(train_lstm1)\n",
    "\n",
    "# 全連接層搭配 Softmax Activation\n",
    "# 可以回傳 10 個成對標題\n",
    "# 屬於各類別的可能機率\n",
    "dense =  Dense(units=NUM_CLASSES, activation='softmax')\n",
    "predictions = dense(train_lstm2)\n",
    "\n",
    "# 我們的模型就是將數字序列的輸入，轉換\n",
    "# 成 3 個分類的機率的所有步驟 / 層的總和\n",
    "model = Model(inputs=train_input,  outputs=predictions)\n",
    "# Loss function\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          verbose=verbosity,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # == Provide average scores ==\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Score per fold')\n",
    "# for i in range(0, len(acc_per_fold)):\n",
    "#     print('------------------------------------------------------------------------')\n",
    "#     print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Average scores for all folds:')\n",
    "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "# print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.argmax(axis=1)\n",
    "pred_test = pd.concat([test_text['id'], pd.DataFrame(predictions.argmax(axis=1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.columns = ['id', 'label']\n",
    "pred_test.to_csv('predv1.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
